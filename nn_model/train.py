import keras
import pandas as pd
import os
import sys
import inspect

from keras.models import Sequential, model_from_json
from keras.layers import Dense, Dropout, Embedding, LSTM, Activation
from keras import optimizers
from sklearn.model_selection import train_test_split

currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
scriptsdir = os.path.join(os.path.dirname(currentdir), "scripts")
sys.path.insert(0, scriptsdir) 

import datahandler as dh
import memoryleak


def load_dataset_btb(path="../data/datasets/byte_to_byte/byte_to_byte.csv",
                        balance=True, randomize=True, normalize=True, test_size=0.2):

    df = pd.read_csv(path)

    
    if balance:
        """
        # Undersample
        g = df.groupby("class")
        df = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))
        """


        # Oversample
        malware_df = list(df.groupby("class"))[1][1]
        while dict(df.groupby("class").size())[0] > dict(df.groupby("class").size())[1]:
            df = pd.concat([df, malware_df])

        g = df.groupby("class")
        df = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))

        print("Amount of safe bytes: {}".format(dict(df.groupby("class").size())[0]))
        print("Amount of malicious bytes: {}".format(dict(df.groupby("class").size())[1]))


    
    if randomize:
        df = df.sample(frac=1)


    if normalize:
        for column in df.columns:
            df[column] = dh.normalize(df[column])

    
    df_train, df_test = train_test_split(df, test_size=test_size)


    x_train = df_train[["byte1", "byte2"]].values
    y_train = df_train[["class"]].values

    x_test = df_test[["byte1", "byte2"]].values
    y_test = df_test[["class"]].values

    num_features = x_train.shape[1]
    output_dim = y_train.shape[1]
    print(num_features, output_dim)

    return (x_train, y_train, x_test, y_test, num_features, output_dim)


def load_dataset_btbs(path="../data/datasets/bytes_to_bytes/bytes_to_bytes.csv",
                        balance=True, randomize=True, normalize=True, test_size=0.2):

    df = pd.read_csv(path)

    
    if balance:
        # Undersample
        #g = df.groupby("class")
        #df = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))


        # Oversample
        malware_df = list(df.groupby("class"))[1][1]
        while dict(df.groupby("class").size())[0] > dict(df.groupby("class").size())[1]:
            df = pd.concat([df, malware_df])

        g = df.groupby("class")
        df = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))

        print("Amount of safe bytes: {}".format(dict(df.groupby("class").size())[0]))
        print("Amount of malicious bytes: {}".format(dict(df.groupby("class").size())[1]))

    
    if randomize:
        df = df.sample(frac=1)


    if normalize:
        for column in df.columns:
            df[column] = dh.normalize(df[column])

    df_train, df_test = train_test_split(df, test_size=test_size)

    del df

    x_train = df_train.drop(columns=["class"]).values
    y_train = df_train[["class"]].values

    x_test = df_test.drop(columns=["class"]).values
    y_test = df_test[["class"]].values

    num_features = x_train.shape[1]
    output_dim = y_train.shape[1]
    print(num_features, output_dim)

    return (x_train, y_train, x_test, y_test, num_features, output_dim)


def load_dataset_elf(path="../data/datasets/elf/", balance=True, randomize=True,
                        normalize=True, test_size=0.2):
    
    df = pd.DataFrame()

    for csv_filename in os.listdir(path):
        print("Reading csv file: {}".format(csv_filename))
        csv_path = os.path.join(path, csv_filename)

        csv_df = pd.read_csv(csv_path)

        df = pd.concat([df, csv_df])

    if balance:
        # Undersample
        #g = df.groupby("class")
        #df = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))


        # Oversample
        malware_df = list(df.groupby("class"))[1][1]
        while dict(df.groupby("class").size())[0] > dict(df.groupby("class").size())[1]:
            df = pd.concat([df, malware_df])

        g = df.groupby("class")
        df = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))

        print("Amount of safe bytes: {}".format(dict(df.groupby("class").size())[0]))
        print("Amount of malicious bytes: {}".format(dict(df.groupby("class").size())[1]))

    
    if randomize:
        df = df.sample(frac=1)


    if normalize:
        for column in df.columns:
            df[column] = dh.normalize(df[column])

    df_train, df_test = train_test_split(df, test_size=test_size)

    del df

    x_train = df_train.drop(columns=["class"]).values
    y_train = df_train[["class"]].values

    x_test = df_test.drop(columns=["class"]).values
    y_test = df_test[["class"]].values

    num_features = x_train.shape[1]
    output_dim = y_train.shape[1]
    print(num_features, output_dim)

    return (x_train, y_train, x_test, y_test, num_features, output_dim)


def create_dense_model(num_features, output_dim, state_size=16, dropout=0.5, loss="binary_crossentropy"):
    model = Sequential()
    model.add(Dense(output_dim * state_size, input_dim=num_features, activation="tanh"))
    model.add(Dropout(dropout))
    model.add(Dense(output_dim * (state_size // 2), activation="tanh"))
    model.add(Dropout(dropout))
    model.add(Dense(output_dim * (state_size // 4), activation="tanh"))
    model.add(Dropout(dropout))
    model.add(Dense(output_dim * (state_size // 8), activation="tanh"))
    model.add(Dropout(dropout))
    model.add(Dense(output_dim, activation="sigmoid"))

    model = compile_model(model, loss=loss)
    print(model.summary())

    return model


def create_lstm_model(num_features, output_dim, dropout=0.5):
    model = Sequential()
    model.add(Embedding(128, num_features))
    model.add(LSTM(32, return_sequences=True))
    model.add(Dropout(dropout))
    model.add(LSTM(4, return_sequences=False))
    model.add(Dropout(dropout))
    model.add(Dense(output_dim, activation="sigmoid"))

    model = compile_model(model)
    print(model.summary())

    return model


def compile_model(model, lr=0.001, loss="binary_crossentropy"):

    rmsprop = optimizers.RMSprop(lr=lr)
    adam = optimizers.Adam(lr=lr)
    model.compile(loss=loss, optimizer=rmsprop, metrics=['accuracy'])

    return model


def train(model, x_train, y_train, epochs=10, batch_size=128):
    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)
    return model


def save_model_file(model, prefix=""):
    model_json = model.to_json()
    with open("{}model.json".format(prefix), "w") as json_file:
        json_file.write(model_json)
    model.save_weights("{}model.h5".format(prefix))

    print("Model saved")


if __name__ == "__main__":
    (x_train, y_train,
        x_test, y_test,
            num_features, output_dim) = load_dataset_elf()

    model = create_dense_model(num_features, output_dim)
    model = train(model, x_train, y_train)

    save_model_file(model)

    score = model.evaluate(x_test, y_test, verbose=1)
    print("Loss {}, accuracy: {}".format(score[0], score[1]))

    #preds = model.predict(x_test)
    #print(preds)