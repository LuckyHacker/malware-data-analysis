import requests
import re
import os
import threading
import time
from bs4 import BeautifulSoup


links_to_crawl = []
crawled_links = []

def link_done():
    crawled_links.append(links_to_crawl[0])
    links_to_crawl.pop(0)


def exe_checker():
    while 1:
        try:
            if ".exe" in links_to_crawl[0]
                print(bcolors.OKGREEN + "DOWNLOADING EXE:" + bcolors.ENDC)
                print(links_to_crawl[0])
                os.system("wget {}".format(links_to_crawl[0]))
            
            link_done()
        except IndexError:
            print("Links to crawl empty")

        time.sleep(0.01)


def search_urls(url):

    try:
        resp = requests.get(url)
    except Exception as e:
        print("Error occurred:")
        print(e)

    soup = BeautifulSoup(resp.text)
    for link in soup.findAll('a', attrs={'href': re.compile("^http://")}):
        links_to_crawl.append(link.get('href'))

    link_done()


class bcolors:

    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m' 
    

if __name__ == "__main__":
    threading.Thread(target=exe_checker).start()